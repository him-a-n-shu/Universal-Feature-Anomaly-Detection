{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the universal feature extractor\n",
    "\n",
    "backbone_arch = models.wide_resnet50_2(weights=None)\n",
    "feature_extractor = nn.Sequential(*list(backbone_arch.children())[:-2]).to(device)\n",
    "\n",
    "feature_extractor.load_state_dict(torch.load(\"universal_feature_extractor.pth\"))\n",
    "feature_extractor.eval()\n",
    "print(\"‚úÖ Universal feature extractor loaded and set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Golden Samples\n",
    "\n",
    "class GoldenSampleDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(image)\n",
    "\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "golden_sample_paths = glob(\"/kaggle/input/mvtec-ad/carpet/train/good/*.png\")\n",
    "golden_dataset = GoldenSampleDataset(golden_sample_paths, inference_transform)\n",
    "golden_loader = DataLoader(golden_dataset, batch_size=32, shuffle=False)\n",
    "print(f\"Prepared {len(golden_dataset)} golden samples for on-site adaptation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "features = {}\n",
    "def get_features_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output\n",
    "    return hook\n",
    "\n",
    "feature_extractor[5].register_forward_hook(get_features_hook('layer2'))\n",
    "feature_extractor[6].register_forward_hook(get_features_hook('layer3'))\n",
    "\n",
    "memory_bank = []\n",
    "print(\"\\nüîç Extracting features from golden samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(golden_loader, desc=\"Feature Extraction\"):\n",
    "        images = images.to(device)\n",
    "        _ = feature_extractor(images)\n",
    "        layer2_features = features['layer2']\n",
    "        layer3_features = features['layer3']\n",
    "        upsampled_layer3 = torch.nn.functional.interpolate(layer3_features, size=layer2_features.shape[2:], mode='bilinear', align_corners=False)\n",
    "        combined_features = torch.cat((layer2_features, upsampled_layer3), dim=1)\n",
    "        patch_embeddings = combined_features.permute(0, 2, 3, 1).flatten(0, 2).cpu().numpy()\n",
    "        memory_bank.append(patch_embeddings)\n",
    "        \n",
    "memory_bank = np.concatenate(memory_bank, axis=0)\n",
    "print(f\"‚úÖ Memory bank created with {memory_bank.shape[0]} feature vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coreset Subsampling\n",
    "\n",
    "def greedy_coreset_subsampling(feature_vectors, percentage=0.01):\n",
    "    n_samples = int(len(feature_vectors) * percentage)\n",
    "    if n_samples == 0: n_samples = 1\n",
    "    \n",
    "    print(f\"\\nüß† Starting coreset subsampling to select {n_samples} representative features...\")\n",
    "    coreset_indices = [np.random.randint(len(feature_vectors))]\n",
    "    min_distances = np.linalg.norm(feature_vectors - feature_vectors[coreset_indices[0]], axis=1)\n",
    "    \n",
    "    progress = tqdm(range(1, n_samples), desc=\"Coreset Subsampling\")\n",
    "    for _ in progress:\n",
    "        next_idx = np.argmax(min_distances)\n",
    "        coreset_indices.append(next_idx)\n",
    "        new_distances = np.linalg.norm(feature_vectors - feature_vectors[next_idx], axis=1)\n",
    "        min_distances = np.minimum(min_distances, new_distances)\n",
    "    return feature_vectors[coreset_indices]\n",
    "\n",
    "coreset = greedy_coreset_subsampling(memory_bank, percentage=0.01)\n",
    "print(f\"‚úÖ Coreset created. Final size: {coreset.shape[0]} feature vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-08T17:33:04.066849Z",
     "iopub.status.busy": "2025-10-08T17:33:04.066564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "‚úÖ Universal feature extractor loaded and set to evaluation mode.\n",
      "Prepared 280 golden samples for on-site adaptation.\n",
      "\n",
      "üîç Extracting features from golden samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d66050d81e845a9a7c994ce1139e5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature Extraction:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory bank created with 219520 feature vectors.\n",
      "\n",
      "üß† Starting coreset subsampling to select 2195 representative features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77d8b067962427ab3686668eb3e84a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Coreset Subsampling:   0%|          | 0/2194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Product Specific Coreset Preparation\n",
    "\n",
    "product_name = \"ENTER_PRODUCT_NAME\"\n",
    "with open(f\"{product_name}_coreset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(coreset, f)\n",
    "    \n",
    "print(f\"\\nüíæ Product-specific memory coreset saved to {product_name}_coreset.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1946896,
     "sourceId": 3209332,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 466058,
     "modelInstanceId": 449681,
     "sourceId": 600205,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
